{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab_backpropagation.ipynb","provenance":[{"file_id":"https://github.com/umbcdata602/fall2020/blob/master/lab_backpropagation.ipynb","timestamp":1604351369560}],"collapsed_sections":[],"authorship_tag":"ABX9TyMeiVQs39SP6nX7EFJH9Mqy"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KPtH6JoCjnsw"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/umbcdata602/fall2020/blob/master/lab_backpropagation.ipynb\">\n","<img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>\n","\n","# Lab -- Backpropagation\n","\n","The goal here is to develop an intuition for backpropagation, with minimal math.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SVWOgkqnjwnC"},"source":["# Supersimplified case\n","\n","The supersimplified model is\n","\n","$$\n","\\hat{y} = wx\n","$$\n","\n","* $x$ the model input (feature)\n","* $\\hat{y}$ is the model prediction of the target variable $y$\n","* $w$ is the trainable weight\n","* let $\\epsilon = \\hat{y} - y$ represent the residual of the model prediction. \n","\n","The cost function $J$\n","$$\n","J = \\epsilon^2 = (y - wx)^2\n","$$\n"," \n","If we had multiple inputs and samples, then $J$ would involve multiple sums, which would make the mathematical manipulations more complicated. We'll hold off.\n","\n","## The solution\n","\n","$J$ is a minimum for \n","\n","$$\n","\\frac{\\partial J}{\\partial w} = 0\n","$$\n","\n","The gradient of $J$ with respect to the weight $w$ is\n","\n","$$\n","\\frac{\\partial J}{\\partial w} = 2(y-wx)(-x) = -2x(y-wx) = -2x\\epsilon\n","$$\n","\n","That is,\n","\n","$$\\epsilon = y - wx = 0$$\n","\n","In this supersimplified case, the solution is obvious by inspection\n","\n","$$\n","w = \\frac{y}{x}\n","$$\n","\n","The model input is $x$, and $y$ is the target output."]},{"cell_type":"markdown","metadata":{"id":"2KVtgmXvkod6"},"source":["# Slightly more complex case\n","\n","We'll add some complexity, and perform the calculations in a sequence of \"Layers.\"\n","\n","Layer 1 multiplies the weight $w$ by the input $x$. The output of Layer 1 is the resulting product $z$,\n","\n","$$\n","z = wx\n","$$\n","\n","Layer 2 takes the output of Layer 1 and computes a nonlinear function $f(z)$. The output of Layer 2 is a prediction $\\hat{y}$ of $y$:\n","\n","$$\n","\\hat{y} = f(z)\n","$$\n","\n","The cost function $J$ can be computed with the result of Layer 2.\n","\n","$$\n","J = (y - f(z))^2 = \\epsilon^2\n","$$\n","\n","The derivative of $J$ wrt the weight $w$ is\n","\n","$$\n","\\frac{\\partial J}{\\partial w} = \n","2(y-f(z))\\left( - \\frac{\\partial f}{\\partial z} \\frac{\\partial z}{\\partial w} \\right) = -2 (y-f(z))\\frac{\\partial f}{\\partial z} \\frac{\\partial z}{\\partial w}\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"9xbi_CcXTJfL"},"source":["## Solving the more complex case\n","\n","To evaluate $\\frac{\\partial J}{\\partial w}$, we need the outputs from each layer, $z$ and $\\hat{y}$.\n","We also need $\\frac{\\partial f}{\\partial z}$ and $\\frac{\\partial z}{\\partial w}$, which we can compute in each layer during the forward pass. That's what happens in practice. The algorithm keeps track of these derivatives during the forward pass in the network. In Tensorflow, that's the purpose of GradientTape.\n","\n","* If $f(z)$ is a linear function of $x$, then $J$ is a parabola as before, and we can solve the problem right away.\n","* If $f(z)$ is a nonlinear function of $x$, then $J$ will have more complex structure. As long as $f(z)$ is a \"well-behaved\" and monotonic function of $z$, then $J$ is convex and the solution occurs where $\\frac{\\partial J}{\\partial w} = 0$, as before.\n","* In general, $J$ can be \"bumpy\" (the figure is from Raschka's [ch12.ipynb](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch12/ch12.ipynb)), in which case a \"noisy\" form of gradient descent can be our friend.\n","\n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch12/images/12_13.png\" width=\"600\"/>\n","\n","* With batch gradient descent, we risk getting caught in a local minimum. That's where the \"noisy\" behavior of stochastic gradient descent can help. As the algorithm progresses down the gradient in search of a global minimum in $J$, SGD can knock the algorithm out of local minima.\n","* The backward pass is comparable to the foward pass in terms of computational expense. And as long as you can take derivatives of the cost function and activation functions, then the backpropagation step can automated."]}]}