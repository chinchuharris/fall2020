{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab_backpropagation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPL1kBxx52XFrJw2WDlkbPe"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KPtH6JoCjnsw"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/umbcdata602/fall2020/blob/master/lab_backpropagation.ipynb\">\n","<img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>\n","\n","# Lab -- Backpropagation\n","\n","The goal here is to develop an intuition for backpropagation, with minimal math.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SVWOgkqnjwnC"},"source":["# Supersimplified case\n","\n","The supersimplified model is\n","\n","$$\n","\\hat{y} = wx\n","$$\n","\n","* $x$ the model input (feature)\n","* $\\hat{y}$ is the model prediction of the target variable $y$\n","* $w$ is the trainable weight\n","* let $\\epsilon = \\hat{y} - y$ represent the residual of the model prediction. \n","\n","The cost function $J$\n","$$\n","J = \\epsilon^2 = (y - wx)^2\n","$$\n"," \n","If we had multiple inputs and samples, then $J$ would involve multiple sums, which would make the mathematical manipulations more complicated. We'll hold off.\n","\n","## The solution\n","\n","$J$ is a minimum for \n","\n","$$\n","\\frac{\\partial J}{\\partial w} = 0\n","$$\n","\n","The gradient of $J$ with respect to the weight $w$ is\n","\n","$$\n","\\frac{\\partial J}{\\partial w} = 2(y-wx)(-x) = -2x(y-wx) = -2x\\epsilon\n","$$\n","\n","That is,\n","\n","$$\\epsilon = y - wx = 0$$\n","\n","In this supersimplified case, the solution is obvious by inspection\n","\n","$$\n","w = \\frac{y}{x}\n","$$\n","\n","The model input is $x$, and $y$ is the target output."]},{"cell_type":"markdown","metadata":{"id":"2KVtgmXvkod6"},"source":["# Slightly more complex case\n","\n","We'll add some complexity, and perform the calculations in a sequence of \"Layers.\"\n","\n","Layer 1 multiplies the weight $w$ by the input $x$. The output of Layer 1 is the resulting product $z$,\n","\n","$$\n","z = wx\n","$$\n","\n","Layer 2 takes the output of Layer 1 and computes a nonlinear function $f(z)$. The output of Layer 2 is a prediction $\\hat{y}$ of $y$:\n","\n","$$\n","\\hat{y} = f(z)\n","$$\n","\n","The cost function $J$ can be computed with the result of Layer 2.\n","\n","$$\n","J = (y - f(z))^2 = \\epsilon^2\n","$$\n","\n","The derivative of $J$ wrt the weight $w$ is\n","\n","$$\n","\\frac{\\partial J}{\\partial w} = \n","2(y-f(z))\\left( - \\frac{\\partial f}{\\partial z} \\frac{\\partial z}{\\partial w} \\right) = -2 (y-f(z))\\frac{\\partial f}{\\partial z} \\frac{\\partial z}{\\partial w}\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"9xbi_CcXTJfL"},"source":["## Solving the more complex case\n","\n","To evaluate $\\frac{\\partial J}{\\partial w}$, we need the outputs from each layer, $z$ and $\\hat{y}$.\n","We also need $\\frac{\\partial f}{\\partial z}$ and $\\frac{\\partial z}{\\partial w}$, which we can compute in each layer during the forward pass. That's what happens in practice. The algorithm keeps track of these derivatives during the forward pass in the network. In Tensorflow, that's the purpose of GradientTape.\n","\n","* If $f(z)$ is a linear function of $x$, then $J$ is a parabola as before, and we can solve the problem right away.\n","* If $f(z)$ is a nonlinear function of $x$, then $J$ will have more complex structure. As long as $f(z)$ is a \"well-behaved\" and monotonic function of $z$, then $J$ is convex and the solution occurs for $\\frac{\\partial J}{\\partial w} = 0$, as before. In that case, we need to solve the equation $y = f(z)$.\n","* In general, however, $J$ may not be convex (see Figure below from Raschka's [ch12.ipynb](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch12/ch12.ipynb)). In this case we perform gradient descent.\n","\n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch12/images/12_13.png\" width=\"600\"/>\n","\n","* If we use batch gradient descent, then we risk getting caught in a local minimum. That's where the \"noisy\" behavior of stochastic gradient descent can be helpful. As the algorithm progresses down the gradient in search of a global minimum in $J$, SGD can knock the algorithm out of local minima."]}]}