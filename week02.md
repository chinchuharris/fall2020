
# Week 2 -- Logistic Regression (8 Sep 2020)

## Notebooks

* [lab02_perceptron.ipynb](https://github.com/umbcdata602/fall2020/blob/master/lab02_perceptron.ipynb)
* [lab03_adaline.ipynb](https://github.com/umbcdata602/fall2020/blob/master/lab03_adaline.ipynb)
* [lab04_logistic_regression.ipynb](https://github.com/umbcdata602/fall2020/blob/master/lab04_logistic_regression.ipynb)

## Assignment

Complete the following 2 exercises before class #3 on 15 Sep:

1. Adaline in 1-D
    * Start with [lab03_adaline.ipynb](https://github.com/umbcdata602/fall2020/blob/master/lab03_adaline.ipynb)
        * We performed classification with 2 classes and 2 features
        * In this assignment, you'll do the relatively simple classification task with 1 feature
    * Perform classification of the iris dataset for setosa and versicolor using petal length.
    * Adapt the procedure in the notebook, and include 3 plots.
    * Create a plot of the data
    * Create a plot of squared error vs epoch
    * Create a third plot that includes all of the following:
        * The data
        * Predictions for a range of inputs that include and extend slightly beyond the data
        * The activation function
        * The decision boundary used to predict class membership
2. Logistic regression in 1-D
    * Start with [lab04_logistic_regression.ipynb](https://github.com/umbcdata602/fall2020/blob/master/lab04_logistic_regression.ipynb)
    * Repeat the analysis above for logistic regression in 1D

## Reading

* Rasckha & Mirjalili, Chapter 3: Tour of ML classifiers using scikit-learn
    * The rest of the chapter
    * Look at associated Jupyter notebooks in github

## Study guide for Week #3

* Stochastic vs batch gradient descent
* SVC vs logistic regression
* Naive Bayes
* Overfitting vs underfitting
* Decision trees
* KNN
* Curse of dimensionality
* Regularization
