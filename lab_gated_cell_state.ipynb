{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab_gated_cell_state.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO9EGksYRTH+hDFTV6dWhi1"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4qEbmdWLqzcC"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/umbcdata602/fall2020/blob/master/lab_gated_cell_state.ipynb\">\n","<img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>\n","\n","# Gated cell state\n","\n","Vanilla RNNs introduce a problem that's solved by gated cells, such as those used in LSTM."]},{"cell_type":"markdown","metadata":{"id":"pqa5H5JPXaFb"},"source":["As Raschka & Mirjalili point out on p577, backpropagation through time (BPTT) introduces a term in \n","\n","$$\n","\\frac{\\partial L^{(t)} }{ \\partial \\mathbf{W}_{hh}}\n","$$\n","\n","that is proportional to\n","\n","$$\n","\\frac{\\partial h^{(t)}}{\\partial h^{(t-k)}}\n","$$\n","for time lag $k$. The recurrence relation in vanilla RNNs gives rise to the problem of vanishing and exploding gradients."]},{"cell_type":"markdown","metadata":{"id":"hlS1FKjKkMt5"},"source":["\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch16/images/16_08.png\" width=\"500px\">\n","\n","To gain a conceptual understanding of the problem, consider this super simplified recurrence relation\n","$$\n","h^{(t)} = \\sigma(wh^{(t-1)})\n","$$\n","which contains the essence of the problem.\n"]},{"cell_type":"markdown","metadata":{"id":"z0ZYYE7mkqIb"},"source":["\n","The derivative for one time step is proportional to $w$,\n","$$\n","\\frac{\\partial h^{(t)}}{\\partial h^{(t-1)}} \n","= w \\sigma ' (wh^{(t-1)})\n","$$\n","where $\\sigma'(x) = \\frac{d\\sigma(x)}{dx} $."]},{"cell_type":"markdown","metadata":{"id":"2P6-NqdjE9Ry"},"source":["\n","\n","\n","\n","For two time steps, we use the chain rule\n","\n","$$\n","\\frac{\\partial h^{(t)}}{\\partial h^{(t-2)}} \n","= w \\sigma'(wh^{(t-1)}) \\frac{\\partial h^{(t-1)}}{\\partial h^{(t-2)}}\n","$$\n","\n","and then the recurrence relation\n","\n","$$\n","\\frac{\\partial h^{(t)}}{\\partial h^{(t-2)}} \n","= w^2 \\sigma'(wh^{(t-1)}) \\sigma'(wh^{(t-2)}) \n","$$\n","\n","to show that the gradient is proportional to $w^2$.\n","In general, the gradient is proportional to $w^k$,\n","\n","$$\n","\\frac{\\partial h^{(t)}}{\\partial h^{(t-k)}} \\propto w^k\n","$$\n","\n","The essence of the problem is that for large $k$ you have either exploding or vanishing gradients, depending whether $|w|>1$ or $|w|<1$, respectively.\n"]},{"cell_type":"markdown","metadata":{"id":"TPt1ZAY5Ww-H"},"source":["<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch16/images/16_09.png\" width=\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"SfNkmHKHGWAH"},"source":["Gated cells avoid the problem with the gated cell state \n","\n","$$c^{(t)} = \n","c^{(t-1)}f(t-1)\n","$$\n","\n","that involves multiplication by the gate function, f(t). For one time step, the gradient is no longer strictly proportional to $w$. \n","$$\n","\\frac{\\partial c^{(t)}}{\\partial c^{(t-1)}} \n","= f(t-1)\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"T0rKDFFZqEu6"},"source":["For two time steps, the recurrence relation yields\n","$$\n","\\frac{\\partial c^{(t)}}{\\partial c^{(t-2)}} \n","= f(t-1)f(t-2)\n","$$\n","\n","And for arbitrary $k$,\n","\n","$$\n","\\frac{\\partial c^{(t)}}{\\partial c^{(t-k)}} \n","= \\prod_{i=0}^k f(t-i)\n","$$\n","\n","The term involving $w^k$ is gone.\n","\n","While it's still possible to get vanishing or exploding gradients, there are now multiple paths to flow information through the network for large values of $k$."]}]}